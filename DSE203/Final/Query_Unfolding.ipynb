{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Query Unfolding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import Datalog_Parsing as dp;"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# read metadictionary.json\n",
    "md = json.loads(open('metadictionary.json').read())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def unfold_datalog_body(datalog, metadictionary):\n",
    "    '''function to unfold datalog string (predicate and atoms) according to mapping metadictionary'''\n",
    "    parsed_dl = dp.parsePredicateAtoms(datalog)\n",
    "    pred = parsed_dl['predicate']\n",
    "    \n",
    "    # create mapping dictionary\n",
    "    mapping_dict = {}\n",
    "    for idx,d in enumerate(metadictionary[pred]['datalog']):\n",
    "        mapping_dict[d] = parsed_dl['atoms'][idx]\n",
    "        \n",
    "    # create source dictionary\n",
    "    src_dict = {}\n",
    "\n",
    "    for idx,m in enumerate(metadictionary[pred]['mapping']):\n",
    "        src = m['source']\n",
    "        src_tbl = m['table']\n",
    "        src_dl = m['source.datalog']\n",
    "\n",
    "        res_dl = ['_'] * len(src_dl)\n",
    "\n",
    "        for idx,s in enumerate(src_dl):\n",
    "            res_dl[idx] = mapping_dict[s]\n",
    "\n",
    "        src_dict[src+'.'+src_tbl] = res_dl\n",
    "    \n",
    "    # convert filled source dictionary to datalog\n",
    "    mapped = ','.join([k+'('+','.join(src_dict[k])+')' for k in src_dict.keys()])\n",
    "    return mapped"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "u'S1.mlfeatures(n,y,month,_,sales,_,_),S2.sentiment(y,month,n,sent)'"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "b = 'mlfeatures(n,y,month,_,sales,_,_,sent)'\n",
    "unfold_datalog_body(b, md)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['mlfeatures']"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ms_relations = ['mlfeatures']\n",
    "ms_relations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ans (nodeId, yr, mn, sales, vol, pm_sales, pm_vol, p3m_sales, p3m_vol, \n",
      "            p12m_sales, p12m_vol, pm_numreviews, pm_avgrating, p3m_numreviews, p3m_avgrating, p12m_numreviews, \n",
      "            p12m_avgrating, pm_avgsntp, p3m_avgsntp, p12m_avgsntp ) :-\n",
      "        mlfeatures ( nodeId, yr, mn, sales, vol, pm_sales, pm_vol, p3m_sales, p3m_vol, p12m_sales, \n",
      "            p12m_vol, pm_numreviews, pm_avgrating, p3m_numreviews, p3m_avgrating, p12m_numreviews, \n",
      "            p12m_avgrating, pm_avgsntp, p3m_avgsntp, p12m_avgsntp ) , \n",
      "        nodeId in (15, 45, 121), \n",
      "        mn=12, \n",
      "        yr=2015.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "u'ans(nodeid,yr,mn,sales,vol,pm_sales,pm_vol,p3m_sales,p3m_vol,p12m_sales,p12m_vol,pm_numreviews,pm_avgrating,p3m_numreviews,p3m_avgrating,p12m_numreviews,p12m_avgrating,pm_avgsntp,p3m_avgsntp,p12m_avgsntp):-s1.mlfeatures(nodeid,yr,mn,sales,vol,pm_sales,pm_vol),s2.sentiment(yr,mn,nodeid,p3m_sales),mn=12,yr=2015,nodeid in (15,45,121)'"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# unfold datalog - unfolds bodies only\n",
    "def unfold_datalog(datalog, metadictionary):\n",
    "    processed = dp.processDatalog(datalog)\n",
    "    processed_unfolded = dp.processDatalog(datalog)\n",
    "    unfolded_datalog_strings = []\n",
    "\n",
    "    for idx,parts in enumerate(processed['single_parts']):\n",
    "        body_unfolded_list = []\n",
    "        for body in parts['body']:\n",
    "            body_unfolded = unfold_datalog_body(body, metadictionary)\n",
    "            body_unfolded_list.extend([body_unfolded])\n",
    "\n",
    "        processed_unfolded['single_parts'][idx]['body'] = body_unfolded_list\n",
    "        unfolded_datalog_strings.extend([dp.buildDatalogString(processed_unfolded['single_parts'][idx])])\n",
    "\n",
    "    unfolded_datalog = '.'.join(unfolded_datalog_strings).lower()\n",
    "    return unfolded_datalog\n",
    "\n",
    "# test using ml_predict_query\n",
    "datalog = dp.ml_predict_query\n",
    "print datalog\n",
    "unfold_datalog(datalog, md)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# test datalog with mediated schema relation as subgoal of groupby\n",
    "datalog  = '''ans (n, y, m, agg_sales) :-\n",
    "    group_by(mlfeatures ( n, y, m, s, v, pm_sales, pm_vol, p3m_sales, p3m_vol, p12m_sales,\n",
    "    p12m_vol, pm_numreviews, pm_avgrating, p3m_numreviews, p3m_avgrating, p12m_numreviews,\n",
    "    p12m_avgrating, pm_avgsntp, p3m_avgsntp, p12m_avgsntp ) , [n], agg_sales=sum(s)),\n",
    "    nodeid in (1,2,3),\n",
    "    m=12,\n",
    "    y=2015.\n",
    "    '''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'mlfeatures_gb_intermed(n,y,m,s,v,pm_sales,pm_vol,p3m_sales,p3m_vol,p12m_sales,p12m_vol,pm_numreviews,pm_avgrating,p3m_numreviews,p3m_avgrating,p12m_numreviews,p12m_avgrating,pm_avgsntp,p3m_avgsntp,p12m_avgsntp):-mlfeatures(n,y,m,s,v,pm_sales,pm_vol,p3m_sales,p3m_vol,p12m_sales,p12m_vol,pm_numreviews,pm_avgrating,p3m_numreviews,p3m_avgrating,p12m_numreviews,p12m_avgrating,pm_avgsntp,p3m_avgsntp,p12m_avgsntp).ans(n,y,m,agg_sales):-group_by(mlfeatures(n,y,m,s,v,pm_sales,pm_vol,p3m_sales,p3m_vol,p12m_sales,p12m_vol,pm_numreviews,pm_avgrating,p3m_numreviews,p3m_avgrating,p12m_numreviews,p12m_avgrating,pm_avgsntp,p3m_avgsntp,p12m_avgsntp),[n],agg_sales=sum(s)),m=12,y=2015,nodeid in (1,2,3)'"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# work in progress - eventually turn into function\n",
    "\n",
    "# turn mediated schema predicates within orderby, groupby, topn into intermediate steps\n",
    "#datalog = dp.analytic_query_1\n",
    "\n",
    "processed = dp.processDatalog(datalog)\n",
    "processed_intermed_step = dp.processDatalog(datalog)\n",
    "\n",
    "datalog_strings = []\n",
    "\n",
    "for idx,part in enumerate(processed['single_parts']):\n",
    "    \n",
    "    gb_parsed = part['groupby.parsed']\n",
    "    \n",
    "    if gb_parsed is not None:\n",
    "        gb_subgoal = part['groupby.parsed']['predicate']\n",
    "        if gb_subgoal['predicate'] in ms_relations:\n",
    "            # build datalog for new intermediate step\n",
    "            gb_pred = gb_subgoal['predicate']\n",
    "            gb_atoms = gb_subgoal['atoms']\n",
    "            intermed_step_body = gb_pred + '(' + ','.join(gb_atoms) + ')'\n",
    "            intermed_step_head = gb_pred + '_gb_intermed(' + ','.join(gb_atoms) + '):-' # how to ensure always unique?\n",
    "            intermed_step_datalog = intermed_step_head + intermed_step_body\n",
    "\n",
    "            # update group by step - use same group by but replace ms_relation subgoal with head of intermediate step\n",
    "            #processed_intermed_step = ...\n",
    "\n",
    "        datalog_strings.extend([intermed_step_datalog, dp.buildDatalogString(part)])\n",
    "        \n",
    "    # do same \"if ob_parsed is not None\" block with orderby, topn\n",
    "    \n",
    "    datalog_result = '.'.join(datalog_strings).lower()\n",
    "    \n",
    "datalog_result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
